{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79eb010a",
   "metadata": {},
   "source": [
    "# 1 - Importing data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987cafa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from varname import argname\n",
    "import re\n",
    "import orjson\n",
    "from os.path import join\n",
    "from src.modules.filestructure import PklPath, OutputPath\n",
    "\n",
    "# Relation dictionary\n",
    "with open(join(PklPath.data_importer, \"df_dict.pkl\"), \"rb\") as pickle_in:\n",
    "    df_dict = pickle.load(pickle_in)\n",
    "\n",
    "with open(join(PklPath.db, \"pk_schema.pkl\"), \"rb\") as pickle_in:\n",
    "    pk_schema = pickle.load(pickle_in)\n",
    "\n",
    "with open(join(PklPath.db, \"db_schema.pkl\"), \"rb\") as pickle_in:\n",
    "    db_schema = pickle.load(pickle_in)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da96277",
   "metadata": {},
   "source": [
    "# 1.2 - Removing empty dataframes\n",
    "Already done that in file No. 0\n",
    "# 1.3 - Replacing NaN to 'null'\n",
    "you can do this at the end, on the result string, but you might get some unique result if a word has null in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa6e931",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dict: dict = {}\n",
    "# looping over all tables in the dictionary\n",
    "for tableName in df_dict.keys():\n",
    "    # creating a copy of the table to do changes on\n",
    "    dfNAN = df_dict[tableName]\n",
    "    # finding all null values and replacing them with the string \"null\"\n",
    "    df = dfNAN.where(pd.notnull(dfNAN), \"null\")\n",
    "    temp_dict.update({tableName: df})\n",
    "\n",
    "# updating the dictionary\n",
    "df_dict = temp_dict\n",
    "del temp_dict, df, dfNAN\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8634ad9",
   "metadata": {},
   "source": [
    "# 2 - Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe2e0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  separate the PK from the table df\n",
    "def _is_blank(myString: str) -> bool:\n",
    "    try:\n",
    "        if type(myString) == tuple or type(myString) == list:\n",
    "            myString = myString[0]\n",
    "    except IndexError:\n",
    "        return True\n",
    "    if myString and myString.strip():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def sub_df(aDF: pd.DataFrame, colList: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function returns the sub columns from a dataframe based on the headers in colList.\n",
    "    \"\"\"\n",
    "    dfList: list = []\n",
    "\n",
    "    for aCol in colList:\n",
    "        if aCol in list(aDF.columns):\n",
    "            dfList.append(aDF[aCol].copy())\n",
    "    return pd.concat(dfList, axis=1)\n",
    "\n",
    "\n",
    "def get_pk(\n",
    "    aTable: pd.DataFrame,\n",
    "    aPK_df: pd.DataFrame,\n",
    "    aTable_str: str = \"\",\n",
    "    ignore_columns: list = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    :param ignore_columns: columns to ignore from the primary keys\n",
    "    :param aTable: a dataframe of the table in question\n",
    "    :param aTable_str: aTable but as a string (this is needed for operations in the function)\n",
    "    :param aPK_df:\n",
    "    :return: Two DataFrames, one is the primary keys of the database table and the other is without the primary keys\n",
    "    \"\"\"\n",
    "    # Populate aTable_str if empty\n",
    "    if _is_blank(aTable_str):\n",
    "        aTable_str: str = argname(\"aTable\")\n",
    "\n",
    "    # 1. pick out the pk from aPK_df -> table_pk_df\n",
    "    table_pk_df = aPK_df.groupby([\"table_name\"]).get_group(aTable_str)\n",
    "\n",
    "    # 2. put all new_pk_df['pk'] in a list -> pk_list\n",
    "    pk_list = list(table_pk_df[\"col_name\"])\n",
    "    # fixme: we are removing the 'EdgeCase' from the list, because it has default value of zero, but it is in the database schema. Need to fix this if I find an EdgeCase.\n",
    "    if type(ignore_columns) != list and ignore_columns is not None:\n",
    "        raise TypeError(\"ignore_columns must be a list of column names to ignore\")\n",
    "    elif ignore_columns is not None:\n",
    "        for col in ignore_columns:\n",
    "            try:\n",
    "                del pk_list[pk_list.index(col)]\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    # 3. separate the pks and non-pks from aTable\n",
    "    aTable_pk_df = sub_df(aTable, pk_list)\n",
    "\n",
    "    # 4. delete pk columns form aTable\n",
    "    aTable_no_pk_df = aTable.drop(pk_list, axis=1)\n",
    "\n",
    "    # 4. return both DFs\n",
    "    return aTable_pk_df, aTable_no_pk_df\n",
    "\n",
    "\n",
    "def getRealType(aHeader: str, aTable: str, type_df: pd.DataFrame) -> \"Class\":\n",
    "    # categorizing the `type_df` based on relation names\n",
    "    table_types = type_df.groupby([\"table_name\"]).get_group(aTable)\n",
    "\n",
    "    # getting the header datatype as the one defined in the database\n",
    "    realType = table_types[table_types[\"col_name\"] == str(aHeader)][\"data_type\"]\n",
    "    if type(realType) == list or type(realType) == tuple:\n",
    "        realType = realType[0]\n",
    "    realType = str(realType)\n",
    "\n",
    "    isText = re.search(r\"varchar\", realType)\n",
    "    isInt = re.search(r\"int\", realType)\n",
    "    isFloat = re.search(r\"decimal\", realType)\n",
    "    isJosn = re.search(r\"json\", realType)\n",
    "\n",
    "    if isText:\n",
    "        return str\n",
    "    if isJosn:\n",
    "        return dict\n",
    "    elif isInt:\n",
    "        return int\n",
    "    elif isFloat:\n",
    "        return float\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f77b32a",
   "metadata": {},
   "source": [
    "# 3 - Creating the SQL code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9981a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: uncomment if the file doesn't have these (they have default values in my sql)\n",
    "ignore_columns = [\"EdgeCase\", \"AliquotID\", \"GlycanSpecimenType\"]\n",
    "# ignore_columns = ['EdgeCase', 'AliquotID']\n",
    "\n",
    "for tableName in list(df_dict.keys()):\n",
    "    print(f\"Working on {tableName} -> \", end=\"\")\n",
    "    aTable = df_dict[tableName]\n",
    "    df_pk, df_no_pk = get_pk(\n",
    "        aTable, pk_schema, tableName, ignore_columns=ignore_columns\n",
    "    )\n",
    "    headers = list(df_no_pk.columns)\n",
    "    headers_pk = list(df_pk.columns)\n",
    "\n",
    "    # loop over rows\n",
    "    for irow in range(df_no_pk.shape[0]):\n",
    "        preText: str = f\"UPDATE HealthProject.{tableName} SET\\n\"\n",
    "        postText: str = \"\\nWHERE \"\n",
    "        values: str = \"\"\n",
    "        final_statement: str = \"\"\n",
    "\n",
    "        # loop over non pk columns\n",
    "        for icol in range(df_no_pk.shape[1]):\n",
    "            dataPoint = df_no_pk.iloc[irow, icol]\n",
    "            real_type = getRealType(headers[icol], tableName, db_schema)\n",
    "\n",
    "            if dataPoint == \"null\":\n",
    "                continue\n",
    "                # values = values + f\"{headers[icol]} = {dataPoint},\\n\"\n",
    "            elif dataPoint != \"null\" and real_type == str:\n",
    "                values = values + f\"{headers[icol]} = '{dataPoint}',\\n\"\n",
    "            elif real_type == int:\n",
    "                values = values + f\"{headers[icol]} = {str(int(dataPoint))},\\n\"\n",
    "            elif real_type == float:\n",
    "                values = values + f\"{headers[icol]} = {str(float(dataPoint))},\\n\"\n",
    "            elif real_type == dict:\n",
    "                if len(dataPoint.keys()) > 0:\n",
    "                    values = (\n",
    "                        values\n",
    "                        + f\"{headers[icol]} = JSON_MERGE_PATCH(IFNULL({headers[icol]},'{{}}'), '{orjson.dumps(dataPoint).decode()}'),\\n\"\n",
    "                    )\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                raise Exception(\n",
    "                    f\"Data type unknown\\n\"\n",
    "                    f\"row num: {irow} ; col num (no pk): {icol}\\n\"\n",
    "                    f\"data point: {dataPoint}\\n\"\n",
    "                    f\"real type: {real_type}\"\n",
    "                )\n",
    "\n",
    "        # testing: if there were nothing to add, we skip that row\n",
    "        if _is_blank(values):\n",
    "            continue\n",
    "        else:\n",
    "            final_statement = preText + values[:-2] + postText\n",
    "            values: str = \"\"\n",
    "\n",
    "        # loop over pk columns\n",
    "        for icol in range(df_pk.shape[1]):\n",
    "            values: str = values + headers_pk[icol] + \" = \"\n",
    "            dataPoint = df_pk.iloc[irow, icol]\n",
    "            real_type = getRealType(headers_pk[icol], tableName, pk_schema)\n",
    "\n",
    "            if dataPoint == \"null\":\n",
    "                values = values + dataPoint + \" AND \"\n",
    "            elif dataPoint != \"null\" and real_type == str:\n",
    "                values = values + \"'\" + dataPoint + \"' AND \"\n",
    "            elif real_type == int:\n",
    "                values = values + str(int(dataPoint)) + \" AND \"\n",
    "            elif real_type == float:\n",
    "                values = values + str(float(dataPoint)) + \" AND \"\n",
    "            else:\n",
    "                raise Exception(\n",
    "                    f\"Primary Key data type unknown\\n\"\n",
    "                    f\"row num: {irow} ; col num (pk): {icol}\\n\"\n",
    "                    f\"data point: {dataPoint}\\n\"\n",
    "                    f\"real type: {real_type}\"\n",
    "                )\n",
    "\n",
    "        final_statement = final_statement + values[:-5] + \";\\n\\n\"\n",
    "\n",
    "        # Export code to file\n",
    "        with open(join(OutputPath.sql, f\"{tableName}_4_update.sql\"), \"a\") as file:\n",
    "            file.write(final_statement)\n",
    "    print(\"Done\")\n",
    "\n",
    "print(\"Finished\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
