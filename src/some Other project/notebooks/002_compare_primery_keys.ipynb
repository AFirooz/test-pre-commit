{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39de18ec",
   "metadata": {},
   "source": [
    "# Importing the data from a dataframe and mySQL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f798c8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "from src.modules.filestructure import PklPath, RunLib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cdd4a6",
   "metadata": {},
   "source": [
    "# 1 - Importing database results as dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25172023",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    RunLib  # this is just so that the import don't get removed\n",
    "    %run $RunLib.db_query_py\n",
    "except Exception:\n",
    "    print(\n",
    "        \"Error: {e}\\nFalling back to previously created pickle files.\\nCheck ReadMe_history.txt for more information.\"\n",
    "    )\n",
    "\n",
    "with open(join(PklPath.db, \"db_schema.pkl\"), \"rb\") as pickle_in:\n",
    "    db_schema: pd.DataFrame = pickle.load(pickle_in)\n",
    "\n",
    "with open(join(PklPath.db, \"pk_schema.pkl\"), \"rb\") as pickle_in:\n",
    "    pk_schema: pd.DataFrame = pickle.load(pickle_in)\n",
    "\n",
    "# Getting the primary keys entries for all relations in the database. This is done to check and not insert a duplicate entry.\n",
    "with open(join(PklPath.db, \"base_df.pkl\"), \"rb\") as pickle_in:\n",
    "    base_df: pd.DataFrame = pickle.load(pickle_in)\n",
    "\n",
    "# Original df\n",
    "with open(join(PklPath.data_importer, \"df.pkl\"), \"rb\") as pickle_in:\n",
    "    df = pickle.load(pickle_in)\n",
    "\n",
    "# Relation dictionary\n",
    "with open(join(PklPath.data_importer, \"df_dict.pkl\"), \"rb\") as pickle_in:\n",
    "    df_dict = pickle.load(pickle_in)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfd2f9e",
   "metadata": {},
   "source": [
    "# 2 - Comparing Primary Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b718196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparePK(\n",
    "    aBase_df: pd.DataFrame,\n",
    "    aDict_df: pd.DataFrame,\n",
    "    aPK_df: pd.DataFrame,\n",
    "    ignore_columns: list = None,\n",
    ") -> \"insert_PK:dict , update_PK:dict\":\n",
    "    \"\"\"\n",
    "    This method takes in the old data (present in the database, called base_df), new data (dictionary result of \"0. editing Excel values\"), and the primary keys dataframe for all tables.\n",
    "    The result will be two dictionaries where their keys are \"table names\" and their values are dataframes of new primary keys that need to be added to the database or updated.\n",
    "\n",
    "    :param ignore_columns: A list of column names to ignore when comparing the primary keys\n",
    "    :param aBase_df: The data acquired from the database\n",
    "    :type aBase_df: Pandas DataFrame\n",
    "    :param aDict_df: The new data that was passed from previous files\n",
    "    :type aDict_df: Pandas DataFrame\n",
    "    :param aPK_df: The primary key information of the database\n",
    "    :type aPK_df: Pandas DataFrame\n",
    "    :return: Two dictionaries (the insert primary keys, and the update primary keys)\n",
    "    \"\"\"\n",
    "\n",
    "    # A list of all db table names\n",
    "    tableNames = set(aDict_df.keys())\n",
    "    run_PersonInfo_check = False\n",
    "    add_PI_to_dict = False\n",
    "    run_SampleInfo_check = False\n",
    "    add_SI_to_dict = False\n",
    "\n",
    "    # making sure PersonInfo (ID) and SampleInfo (ID and AliqoutID) are either in dDict_df or if not, checked for new primary keys\n",
    "    if \"SampleInfo\" not in tableNames and aBase_df is not None:\n",
    "        run_SampleInfo_check = True\n",
    "        # TODO: create a function that would return all PKs for a table so we don't clutter this function\n",
    "        sample_pks = (\n",
    "            base_df.loc[:, (\"SampleInfo_ID\", \"SampleInfo_AliquotID\")]\n",
    "            .copy()\n",
    "            .dropna()\n",
    "            .applymap(int)\n",
    "            .rename({\"SampleInfo_ID\": \"ID\", \"SampleInfo_AliquotID\": \"AliquotID\"})\n",
    "        )\n",
    "        insert_SampleInfo_series = pd.DataFrame()\n",
    "        if sample_pks.empty:\n",
    "            run_SampleInfo_check = False\n",
    "\n",
    "    if \"PersonInfo\" not in tableNames and aBase_df is not None:\n",
    "        run_PersonInfo_check = True\n",
    "        person_pks = (\n",
    "            base_df.loc[:, \"PersonInfo_ID\"].copy().dropna().map(int).rename(\"ID\")\n",
    "        )\n",
    "        insert_PersonInfo_series = pd.Series(dtype=\"float64\")\n",
    "        if person_pks.empty:\n",
    "            run_PersonInfo_check = False\n",
    "\n",
    "    # initiating the returned values\n",
    "    insert_pk: dict = {}\n",
    "    update_pk: dict = {}\n",
    "\n",
    "    # Checking the primary keys of each table to itself\n",
    "    # 0. skipping the check if aBase is None (the database is empty)\n",
    "    if aBase_df is None:\n",
    "        # Then all the primary keys are new and must add them all\n",
    "        # 1. Looping over all tables\n",
    "        for aTable in tableNames:\n",
    "            # 1.1 list the primary keys of aTable\n",
    "            aTable_pk = list(aPK_df[aPK_df[\"table_name\"] == aTable][\"col_name\"])\n",
    "\n",
    "            # making sure that ignore_columns is a list if not None\n",
    "            if type(ignore_columns) != list and ignore_columns is not None:\n",
    "                raise TypeError(\n",
    "                    \"ignore_columns must be a list of column names to ignore when comparing the primary keys\"\n",
    "                )\n",
    "            elif ignore_columns is not None:\n",
    "                aTable_pk = list(set(aTable_pk) - set(ignore_columns))\n",
    "\n",
    "            # 2. get the primary keys of that table from df_dict (skipping if a tabel only has the primary keys)\n",
    "            if set(aDict_df[aTable].columns) - set(aTable_pk) != set():\n",
    "                insert_df = aDict_df[aTable].loc[:, aTable_pk]\n",
    "\n",
    "                # resetting the dataframe index, just in case\n",
    "                insert_df.reset_index(inplace=True)\n",
    "                insert_df.drop(labels=\"index\", axis=1, inplace=True)\n",
    "\n",
    "                # 5. Saving the new PKs in a dict\n",
    "                insert_pk.update({aTable: insert_df})\n",
    "            else:\n",
    "                continue\n",
    "    else:\n",
    "        # 1. Looping over all tables\n",
    "        for aTable in tableNames:\n",
    "            # 1.1 list the primary keys of aTable\n",
    "            aTable_pk = list(aPK_df[aPK_df[\"table_name\"] == aTable][\"col_name\"])\n",
    "\n",
    "            # making sure that ignore_columns is a list if not None\n",
    "            if type(ignore_columns) != list and ignore_columns is not None:\n",
    "                raise TypeError(\n",
    "                    \"ignore_columns must be a list of column names to ignore when comparing the primary keys\"\n",
    "                )\n",
    "            # removing the columns to ignore from the primary keys\n",
    "            elif ignore_columns is not None:\n",
    "                aTable_pk = list(set(aTable_pk) - set(ignore_columns))\n",
    "\n",
    "            # 2. get the primary keys of that table from df_dict (skipping if a tabel only has the primary keys)\n",
    "            if set(aDict_df[aTable].columns) - set(aTable_pk) != set():\n",
    "                query_pk = aDict_df[aTable].loc[:, aTable_pk]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # 3. get the primary key entries from base_df\n",
    "            ## creating column names to match base_pk (like tableName_columnName)\n",
    "            base_pk_names: list = [str(f\"{aTable}_{element}\") for element in aTable_pk]\n",
    "            ## getting the primary key rows that are related to aTable and already in the database\n",
    "            base_pk = aBase_df.loc[:, base_pk_names].dropna(subset=[(aTable + \"_ID\")])\n",
    "\n",
    "            # Renaming base_pk columns to match query_pk and changing the data type\n",
    "            # TODO: see if you can use the function map() or applymap() instead of looping\n",
    "            for i, element in enumerate(aTable_pk):\n",
    "                base_pk.rename(inplace=True, columns={base_pk_names[i]: element})\n",
    "                # changing the data type to int\n",
    "                PK_dataType: str = str(\n",
    "                    list(aPK_df[aPK_df[\"col_name\"] == element][\"data_type\"])\n",
    "                )\n",
    "                isText = re.search(\"varchar*\", PK_dataType)\n",
    "                if isText:\n",
    "                    pass\n",
    "                else:\n",
    "                    base_pk[element] = base_pk[element].astype(int)\n",
    "\n",
    "            # 4. compare primary keys (Merge DataFrames with indicator)\n",
    "            result_df = base_pk.merge(query_pk, indicator=True, how=\"outer\")\n",
    "            insert_df = result_df[result_df[\"_merge\"] == \"right_only\"].drop(\n",
    "                columns=[\"_merge\"]\n",
    "            )\n",
    "            update_df = result_df[result_df[\"_merge\"] == \"both\"].drop(\n",
    "                columns=[\"_merge\"]\n",
    "            )\n",
    "\n",
    "            # resetting the dataframe index, just in case\n",
    "            insert_df.reset_index(inplace=True)\n",
    "            insert_df.drop(labels=\"index\", axis=1, inplace=True)\n",
    "\n",
    "            # 5. Saving the new PKs in a dict\n",
    "            insert_pk.update({aTable: insert_df})\n",
    "\n",
    "            if len(update_df) > 0:\n",
    "                # resetting the dataframe index, just in case, before adding the dataframe\n",
    "                update_df.reset_index(inplace=True)\n",
    "                update_df.drop(labels=\"index\", axis=1, inplace=True)\n",
    "                update_pk.update({aTable: update_df})\n",
    "\n",
    "            # 6. check if we need to insert any new IDs in PersonInfo or SampleInfo\n",
    "            if run_PersonInfo_check:\n",
    "                add_PI_to_dict = True\n",
    "                temp_df = pd.merge(\n",
    "                    person_pks,\n",
    "                    query_pk.loc[:, \"ID\"],\n",
    "                    on=\"ID\",\n",
    "                    indicator=True,\n",
    "                    how=\"outer\",\n",
    "                )\n",
    "                insert_PersonInfo_df = temp_df[temp_df[\"_merge\"] == \"right_only\"].drop(\n",
    "                    columns=[\"_merge\"]\n",
    "                )\n",
    "                insert_PersonInfo_df.reset_index(inplace=True)\n",
    "                insert_PersonInfo_df.drop(labels=\"index\", axis=1, inplace=True)\n",
    "                insert_PersonInfo_series = pd.concat(\n",
    "                    [insert_PersonInfo_series, insert_PersonInfo_df]\n",
    "                )\n",
    "\n",
    "            if run_SampleInfo_check and \"AliquotID\" in list(query_pk.columns):\n",
    "                add_SI_to_dict = True\n",
    "                temp_df = sample_pks.merge(\n",
    "                    query_pk.loc[:, (\"ID\", \"AliquotID\")], indicator=True, how=\"outer\"\n",
    "                )\n",
    "                insert_SampleInfo_df = temp_df[temp_df[\"_merge\"] == \"right_only\"].drop(\n",
    "                    columns=[\"_merge\"]\n",
    "                )\n",
    "                insert_SampleInfo_df.reset_index(inplace=True)\n",
    "                insert_SampleInfo_df.drop(labels=\"index\", axis=1, inplace=True)\n",
    "                insert_SampleInfo_series = pd.concat(\n",
    "                    [insert_SampleInfo_series, insert_SampleInfo_df]\n",
    "                )\n",
    "\n",
    "        # adding PersonalInfo and SampleInfo to the insert dictionary\n",
    "        if run_PersonInfo_check and len(insert_PersonInfo_series) > 0:\n",
    "            insert_PersonInfo_series = (\n",
    "                insert_PersonInfo_series.drop(columns=0)\n",
    "                .drop_duplicates()\n",
    "                .reset_index()\n",
    "                .drop(columns=\"index\")\n",
    "                .astype(int)\n",
    "            )\n",
    "        if run_SampleInfo_check and len(insert_SampleInfo_series) > 0:\n",
    "            insert_SampleInfo_series = (\n",
    "                insert_SampleInfo_series.drop_duplicates()\n",
    "                .reset_index()\n",
    "                .drop(columns=\"index\")\n",
    "                .astype(int)\n",
    "            )  # fixme: need to make sure this is correct !\n",
    "        if add_PI_to_dict and len(insert_PersonInfo_series) > 0:\n",
    "            # insert_PersonInfo_series.drop(labels='index', axis=1, inplace=True)\n",
    "            insert_pk.update({\"PersonInfo\": insert_PersonInfo_series})\n",
    "        if add_SI_to_dict and len(insert_SampleInfo_series):\n",
    "            insert_pk.update({\"SampleInfo\": insert_SampleInfo_series})\n",
    "\n",
    "    return insert_pk, update_pk\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994a4a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_PK, update_PK = comparePK(\n",
    "    base_df,\n",
    "    df_dict,\n",
    "    pk_schema,\n",
    "    ignore_columns=[\"EdgeCase\", \"GlycanSpecimenType\", \"AliquotID\"],\n",
    ")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3169aa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixme: need to make sure this is correct if you have AliquotIDs. look for \"fixme\" in the \"comparePK()\" function!\n",
    "# insert_PK['SampleInfo']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ea0211",
   "metadata": {},
   "source": [
    "# 6 - Exporting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648cf3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = True\n",
    "for name in [\"2_insertPK\", \"2_updatePK\"]:\n",
    "    test = glob(join(PklPath.data_importer, f\"{name}.pkl\"))\n",
    "    if test:\n",
    "        overwrite = input(f\"{name} exists, Overwrite? (y/n)\").lower() == \"y\"\n",
    "        if not overwrite:\n",
    "            break\n",
    "        # todo: make it check each one independently\n",
    "\n",
    "if overwrite:\n",
    "    with open(join(PklPath.data_importer, \"2_insertPK.pkl\"), \"wb\") as pickle_out:\n",
    "        pickle.dump(insert_PK, pickle_out)\n",
    "\n",
    "    with open(join(PklPath.data_importer, \"2_updatePK.pkl\"), \"wb\") as pickle_out:\n",
    "        pickle.dump(update_PK, pickle_out)\n",
    "\n",
    "else:\n",
    "    raise Exception(\"Allow overwriting, or rename\")\n",
    "\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
