{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Getting DB information\n",
    "## Getting all tables, all columns data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: put all functions in a file and just import it !\n",
    "# TODO: Use os.path to check and not override saving things\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from varname import argname, nameof\n",
    "import pickle\n",
    "import re\n",
    "from typing import Type\n",
    "import orjson as json\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "import os.path as path\n",
    "\n",
    "import filestructure as fs\n",
    "import importer as imp\n",
    "from filestructure import PklPath, RunLib\n",
    "from importer import sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing database results as dataframes\n",
    "try:\n",
    "    RunLib  # this is just so that the import statement doesn't get removed\n",
    "    %run $RunLib.db_query_py\n",
    "except Exception:\n",
    "    print(\n",
    "        \"Error:\\nFalling back to previously created pickle files.\\nCheck ReadMe_history.txt for more information.\"\n",
    "    )\n",
    "\n",
    "with open(join(PklPath.db, \"db_schema.pkl\"), \"rb\") as pickle_in:\n",
    "    db_schema: pd.DataFrame = pickle.load(pickle_in)\n",
    "\n",
    "with open(join(PklPath.db, \"pk_schema.pkl\"), \"rb\") as pickle_in:\n",
    "    pk_schema: pd.DataFrame = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_df(\n",
    "    aDF: pd.DataFrame, aTable: str, aSchema_df: pd.DataFrame = db_schema\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    A function that takes a name of a table, retrieve its columns from the database schema, and test each column name for existence in the cluttered DataFrame. When found, it separates it and finally, it will join all columns together and returns them as a one DataFrame.\n",
    "    :param aDF: A disordered / cluttered DataFrame\n",
    "    :param aTable: A table name in as it appears in the database\n",
    "    :param aSchema_df: A database schema as a DataFrame\n",
    "    :return: A Pandas DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # making sure aSchema_df is not empty\n",
    "    if db_schema.empty:\n",
    "        raise Exception(\"The database schema DataFrame is empty\")\n",
    "\n",
    "    colList: list = list(aSchema_df[aSchema_df[\"table_name\"] == aTable][\"col_name\"])\n",
    "    dfList: list = []\n",
    "\n",
    "    for aCol in colList:\n",
    "        if aCol in list(aDF.columns):\n",
    "            dfList.append(aDF[aCol].copy())\n",
    "    return pd.concat(dfList, axis=1)\n",
    "\n",
    "\n",
    "def getPrimaryKey(\n",
    "    aTable: str,\n",
    "    aPK_df: pd.DataFrame = pk_schema,\n",
    "    ignore_columns: list = None,\n",
    "    include_columns: list = None,\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    A function that takes a table name (as it appears in the database) as a string, and optionally the database primary keys schema as a DataFrame, and returns a list of the table's primary keys.\n",
    "    :param include_columns: Columns to include in the primary key list\n",
    "    :param ignore_columns: A list of columns to ignore\n",
    "    :param aTable: A table name in as it appears in the database\n",
    "    :param aPK_df: A database primary keys schema as a DataFrame\n",
    "    :return: list of primary keys\n",
    "    \"\"\"\n",
    "\n",
    "    # making sure that aTable is a table name, so later we can use eval() with lower security risk\n",
    "    if aTable not in set(db_schema[\"table_name\"]):\n",
    "        raise Exception(f\"Enter a valid table name, {aTable} is not valid!\")\n",
    "\n",
    "    # if the table don't have any data other than the primary keys, we just return something basic, so we don't break the code.\n",
    "    # we are assuming that if we have table with data, it will have at least 3 columns, including the primary key.\n",
    "    if len(eval(aTable).columns) < 2:\n",
    "        return [\"ID\"]\n",
    "\n",
    "    # getting the primary key\n",
    "    # if aPK_df is pk_schema and pk_schema.empty:\n",
    "    if type(aPK_df) == pd.DataFrame and aPK_df.empty:\n",
    "        raise Exception(\"There is no primary key found in the database\")\n",
    "    elif type(aPK_df) != pd.DataFrame:\n",
    "        raise Exception(\"Enter a valid DataFrame for the primary keys schema\")\n",
    "    else:\n",
    "        pk_list = list(aPK_df[aPK_df[\"table_name\"] == aTable][\"col_name\"])\n",
    "\n",
    "        # making sure that ignore_columns is a list if not None\n",
    "        if type(ignore_columns) != list and ignore_columns is not None:\n",
    "            raise TypeError(\"ignore_columns must be a list of column names to ignore\")\n",
    "        elif ignore_columns is not None:\n",
    "            pk_list = list(set(pk_list) - set(ignore_columns))\n",
    "\n",
    "        if type(include_columns) != list and include_columns is not None:\n",
    "            raise TypeError(\"include_columns must be a list of column names\")\n",
    "        elif include_columns is not None:\n",
    "            pk_list = pk_list + include_columns\n",
    "\n",
    "        return pk_list\n",
    "\n",
    "\n",
    "def print_pretty_json(data: str = None) -> None:\n",
    "    \"\"\"\n",
    "    Prints pretty JSON values\n",
    "    :param data: a json / str\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if len(data) == 0 or data is None:\n",
    "        print(\"JSON values are empty!\")\n",
    "        return\n",
    "    data = json.loads(data)\n",
    "    print(json.dumps(data, option=json.OPT_INDENT_2).decode())\n",
    "\n",
    "\n",
    "def str_to_dict(\n",
    "    data: str,\n",
    "    data_json_name: str = \"data\",\n",
    "    include_count: bool = True,\n",
    "    keep_original: bool = False,\n",
    "    cast_data: Type = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    This function is used to create a dictionary of the data then it will be converted to JSON values and returned as a string.\n",
    "    Case Usage:\n",
    "    1. This function will split the gene mutations, put them in an array, and assign the key 'mutations' to them. Also, it will count them and add a 'count' key.\n",
    "    2. In Pathways, it will just take the data, convert to json and return\n",
    "    :param cast_data: cast the data to a specific type\n",
    "    :param keep_original: To keep the original data and not cast it to a string\n",
    "    :param data_json_name: The name of key to save the data into\n",
    "    :param include_count: If 'count' key should be included in the json value\n",
    "    :param return_bytes: To specify the return value type\n",
    "    :param data: Data to parse into json\n",
    "    :type data: String\n",
    "    :return: Either a string or bytes of the result json value\n",
    "    \"\"\"\n",
    "    if data is None or pd.isna(data):\n",
    "        return np.NaN\n",
    "    if keep_original:\n",
    "        if cast_data is not None:\n",
    "            data = cast_data(data)\n",
    "        data_blocks = data\n",
    "    else:\n",
    "        data_blocks = str(data).replace(\" \", \"\").split(\",\")\n",
    "        if cast_data is not None:\n",
    "            for i, block in enumerate(data_blocks):\n",
    "                data_blocks[i] = cast_data(block)\n",
    "\n",
    "    if include_count:\n",
    "        # result = json.dumps({str(data_json_name): data_blocks, 'count':len(data_blocks)}\n",
    "        return {str(data_json_name): data_blocks, \"count\": len(data_blocks)}\n",
    "    else:\n",
    "        # result = json.dumps({str(data_json_name): data_blocks})\n",
    "        return {str(data_json_name): data_blocks}\n",
    "\n",
    "\n",
    "def _clean_nulls_helper(value: dict or list) -> dict or [dict]:\n",
    "    \"\"\"\n",
    "    Recursively remove all None values from dictionaries and lists, and returns\n",
    "    the result as a new dictionary or list.\n",
    "    source: https://stackoverflow.com/questions/4255400/exclude-empty-null-values-from-json-serialization\n",
    "    \"\"\"\n",
    "    if isinstance(value, list):\n",
    "        return [_clean_nulls_helper(x) for x in value if x is not None]\n",
    "    elif isinstance(value, dict):\n",
    "        return {\n",
    "            key: _clean_nulls_helper(val)\n",
    "            for key, val in value.items()\n",
    "            if val is not None\n",
    "        }\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "\n",
    "def clean_nulls(data: dict or str) -> dict:\n",
    "    \"\"\"\n",
    "    Remove all None values from dictionaries and lists, and returns\n",
    "    the result as a new dictionary or list.\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        raise ValueError(\"data must not be None\")\n",
    "    if type(data) is str:\n",
    "        result = _clean_nulls_helper(json.loads(data))\n",
    "    else:\n",
    "        result = _clean_nulls_helper(data)\n",
    "        # if you want the return to be a string, uncomment the following line\n",
    "        # result = json.dumps(result).decode()\n",
    "    return result\n",
    "\n",
    "\n",
    "#  This function can find duplicates, return them, and drop them if needed.\n",
    "# TODO: add functionality to the function to create a df['EdgeCase']=np.ones(len(subtracted_d)) when told to. This is helpful for Genes table. Let's worry about it when we need to.\n",
    "def dropDup(\n",
    "    aTable: str,\n",
    "    drop_full_dup=True,\n",
    "    drop_pk_dup=False,\n",
    "    return_dup=False,\n",
    "    keep_dup=False,\n",
    "    ignore_columns: list = None,\n",
    "    include_columns: list = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    A function to check for duplicate entries in a database using the primary keys of that table as a subset. You pass it the table name you are interested in checking and specify any additional parameters, or just keep them as default.\n",
    "    :param include_columns: columns to include in the check for duplicates\n",
    "    :param ignore_columns: A list of columns to ignore when checking for duplicates\n",
    "    :param aTable: Table name\n",
    "    :param drop_pk_dup: Drop rows that have duplicated primary keys\n",
    "    :param drop_full_dup: Drop rows that are fully duplicated\n",
    "    :param return_dup: return the duplicated rows found\n",
    "    :param keep_dup: This is Pandas.duplicated() argument, meaning which duplicated rows to keep.\n",
    "    :return: A DataFrame with (or without) duplicate rows\n",
    "    \"\"\"\n",
    "\n",
    "    # making sure that aTable is a table name, so later we can use eval() with lower security risk\n",
    "    if aTable not in set(db_schema[\"table_name\"]):\n",
    "        raise Exception(f\"Enter a valid table name, {aTable} is not valid!\")\n",
    "\n",
    "    # if the table don't have enough data, we just return something basic, so we don't break the code. These tables will be eliminated in following steps.\n",
    "    if len(eval(aTable).columns) == 1:\n",
    "        return eval(aTable)\n",
    "\n",
    "    # if a table have json type values as dictionary, we need to convert them to a hashable data structure first (like tuples), then drop duplicates\n",
    "    # Notice that we don't need to convert them back to a dictionary since we either notice duplicates in primary keys and rais an exception, or choose to drop primary key duplicates, and then we wouldn't need to check dictionary values.\n",
    "    aTable_hashed = eval(aTable).copy()\n",
    "    json_cols = set(db_schema.groupby(\"data_type\").get_group(\"json\")[\"col_name\"])\n",
    "    for col in aTable_hashed.columns:\n",
    "        if col in json_cols:\n",
    "            aTable_hashed.loc[:, col] = aTable_hashed.loc[:, col].map(\n",
    "                lambda x: json.dumps(x), na_action=\"ignore\"\n",
    "            )\n",
    "\n",
    "    # getting primary keys\n",
    "    temp_keys: list = getPrimaryKey(\n",
    "        aTable, ignore_columns=ignore_columns, include_columns=include_columns\n",
    "    )\n",
    "\n",
    "    # Either dropping rows that are fully duplicate and then check for duplicate primary key rows, or just skipping the first step\n",
    "    # if we plan on dropping duplicated primary key rows, there is no need to drop fully duplicated row since we will eventually do it.\n",
    "    if not drop_pk_dup:  # drop_pk_dup == False\n",
    "        if drop_full_dup:\n",
    "            # dropping fully duplicated rows\n",
    "            temp_df = aTable_hashed.drop_duplicates(ignore_index=True, keep=\"first\")\n",
    "            # checking for duplicate primary keys\n",
    "            temp_dup_df = temp_df[temp_df.duplicated(subset=temp_keys, keep=keep_dup)]\n",
    "        else:\n",
    "            # if we don't want to drop fully duplicated rows, we'll just check for duplicate primary keys\n",
    "            temp_dup_df = eval(aTable)[\n",
    "                eval(aTable).duplicated(subset=temp_keys, keep=keep_dup)\n",
    "            ]\n",
    "\n",
    "    # notifying the user of duplicate key existence\n",
    "    # notice that if we are going to drop duplicated primary keys, then we don't need to raise an error about them\n",
    "    if len(temp_dup_df) > 0 and not drop_pk_dup:  # drop_pk_dup == False\n",
    "        if return_dup:\n",
    "            return temp_dup_df  # if you don't want the dictionaries in a table, you will need to convert them back to a dictionary\n",
    "        display(temp_dup_df)\n",
    "        raise Exception(\n",
    "            f\"You have duplicated primary keys entry in table {aTable}.\\nYou can start by returning the duplicated rows and maybe set drop_duplicates argument to True\"\n",
    "        )\n",
    "\n",
    "    # if no duplicates, or if we choose to drop them we run this\n",
    "    else:  # same as saying (elif drop_pk_dup or len(temp_dup_df) == 0:)\n",
    "        try:\n",
    "            return eval(aTable).drop_duplicates(\n",
    "                subset=temp_keys, ignore_index=True, keep=\"first\"\n",
    "            )\n",
    "        except KeyError:\n",
    "            print(f\"Table {aTable} -> Error: Primary key was not found\")\n",
    "\n",
    "\n",
    "def _is_blank(myString: str) -> bool:\n",
    "    \"\"\"\n",
    "    A helping function to check if a string given as an input is empty or blank. Also, there is a check to make sure if the input was a tuple or a list of one string, it would take that string and do not return false.\n",
    "    Note that empty (\"\") and blank (\"  \") strings and `None` objects are different things, but all mean that the string is not there or empty. Hence,, the use of `str.strip()` method.\n",
    "    Inspired by stackoverflow.com discussion:\n",
    "    https://stackoverflow.com/questions/9573244/how-to-check-if-the-string-is-empty\n",
    "\n",
    "    :param myString: A string to check if it is empty or blank\n",
    "    :return: boolean value\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # just in case myString wasn't a string\n",
    "        if type(myString) == tuple or type(myString) == list:\n",
    "            myString = myString[0]\n",
    "    except IndexError:\n",
    "        return True\n",
    "\n",
    "    # checking if myString is empty\n",
    "    if myString and myString.strip():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def check_dtype(\n",
    "    aTable: pd.DataFrame, aHeader: str, dataType: Type, ignoreDataType: Type = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    This function accepts a dataframe and a header name along with the expected data type that should be stored in that column, and optionaly data types that should be ignored. The function will loop over each row and examine the aHeader data and make sure it matched the type given. If in case it didn't, the function will save that data in another dataframe along with its position index (not index name) and type. The function will finally check if there is any entry that didn't match, and will rais an exception requiring the user the fix it, otherwise, the function will just exit.\n",
    "\n",
    "    :param aTable: The DataFrame containing the column to check.\n",
    "    :param aHeader: The name of the column to check.\n",
    "    :param dataType: The expected data type of the column.\n",
    "    :param ignoreDataType: A data type to ignore when checking the column, defaults to None.\n",
    "\n",
    "    :raises Exception: If the data type of any data in the column does not match the specified data type or is not of the specified ignored data type.\n",
    "    \"\"\"\n",
    "    # a temp dataframe to store wrong data that have wrong datatype\n",
    "    temp_df = pd.DataFrame({\"rowNum\": [], \"WrongType\": [], \"ActualValue\": []})\n",
    "\n",
    "    # looping over all the data and checking their types one by one\n",
    "    for i, data in enumerate(aTable[aHeader]):\n",
    "        # we saved it in a temp variable so that if need be we use regular expression to find the truth\n",
    "        temp = type(data)\n",
    "\n",
    "        # if the data type don't match what we are looking for, or what we should ignore, that data gets appended to the temp_df. Otherwise, we just move on to check the next row.\n",
    "        if temp != dataType and str(data) != \"nan\":\n",
    "            if ignoreDataType != None and temp == ignoreDataType:\n",
    "                continue\n",
    "            temp_df.loc[len(temp_df)] = [i, temp.__name__, data]\n",
    "\n",
    "    # return an error if we found a problem\n",
    "    if len(temp_df) != 0:\n",
    "        otherTypes = set(temp_df[\"WrongType\"])\n",
    "        print(\n",
    "            f\"Not valid types found in the {aHeader} header -> {otherTypes} \\nShould've been {dataType}\"\n",
    "        )\n",
    "        print(\"------------------------------------------\")\n",
    "        print(f\"The problem rows was found in: \\n{temp_df}\")\n",
    "        raise Exception(f\"You need to fix all rows in {aHeader} to match {dataType}\")\n",
    "\n",
    "\n",
    "def verify_dtype(\n",
    "    aTable: pd.DataFrame,\n",
    "    aDB_schema: pd.DataFrame,\n",
    "    aTable_str: str = \"\",\n",
    "    ignoreHeaders: tuple = None,\n",
    ") -> None:\n",
    "    # this function is very similar to `getRealType()` in file no. 5\n",
    "    \"\"\"\n",
    "    This function takes in a table (dataframe) to loop through its headers and grab each one of them, then it grabs datat type of that same header from the database schema table (dataframe), and compare all the columns data types with the database's one. If you have columns that you know will not be in the database directly, like columns that will be used to create JSON values, you can add them to the ignore list.\n",
    "\n",
    "    :param ignoreHeaders: A tuple of headers names that you want to ignore when checking and verifying the types. This is mostly used for data that will be converted to JSON later on.\n",
    "    :type ignoreHeaders: tuple\n",
    "\n",
    "    :param aTable: The DataFrame to verify the column data types for.\n",
    "    :type aTable: pd.DataFrame\n",
    "\n",
    "    :param aDB_schema: The DataFrame containing the column data types to compare against.\n",
    "    :type aDB_schema: pd.DataFrame\n",
    "\n",
    "    :param aTable_str: The name of the table in the aDB_schema DataFrame to compare against. If not provided, it will try to infer the name from the variable name of aTable.\n",
    "    :type aTable_str: str, optional\n",
    "\n",
    "    :return: A dictionary containing the column names and their verified data types.\n",
    "    :rtype: dict\n",
    "\n",
    "    :raises Exception: If the function is unable to match a column name to any column in the aDB_schema DataFrame or if the data type of a column in aTable does not match the corresponding column in aDB_schema.\n",
    "    \"\"\"\n",
    "    print(f\"Table {aTable_str} -> \", end=\"\")\n",
    "\n",
    "    # making sure that \"ignoreHeaders\" is a list\n",
    "    if ignoreHeaders is None or len(ignoreHeaders) == 0:\n",
    "        ignoreHeaders = ()\n",
    "\n",
    "    # Populate aTable_str if empty\n",
    "    if _is_blank(aTable_str):\n",
    "        aTable_str: str = argname(\"aTable\")\n",
    "\n",
    "    # Getting the types of data stored in aTable columns\n",
    "    aTable_types = aDB_schema.groupby([\"table_name\"]).get_group(aTable_str)\n",
    "\n",
    "    # aTable's header names\n",
    "    headers: list = list(aTable.columns)\n",
    "    headersType: dict = {}\n",
    "    for aHeader in headers:\n",
    "        realType = aTable_types[aTable_types[\"col_name\"] == str(aHeader)][\"data_type\"]\n",
    "\n",
    "        # just in case we get a tuple or a list with one item, we grab that one item and don't break the code\n",
    "        if type(realType) == list or type(realType) == tuple:\n",
    "            realType = realType[0]\n",
    "        realType = str(realType)\n",
    "\n",
    "        # using RE to find out if it is text, number, or float\n",
    "        # Since we already converted relevant columns to JSON, we need to figure out how to check the data in them\n",
    "        isText = re.search(r\"varchar\", realType)\n",
    "        isInt = re.search(r\"int\", realType)\n",
    "        isFloat = re.search(r\"decimal\", realType)\n",
    "        isJson = re.search(r\"json\", realType)\n",
    "\n",
    "        if isText:\n",
    "            check_dtype(aTable, aHeader, str)\n",
    "            # headersType.update({str(aHeader) : str})\n",
    "        elif isInt:\n",
    "            check_dtype(aTable, aHeader, int, float)\n",
    "            # headersType.update({str(aHeader) : int})\n",
    "        elif isFloat:\n",
    "            check_dtype(aTable, aHeader, float, int)\n",
    "            # headersType.update({str(aHeader) : float})\n",
    "        elif isJson:\n",
    "            check_dtype(aTable, aHeader, dict)\n",
    "        elif aHeader in ignoreHeaders:\n",
    "            continue\n",
    "        else:\n",
    "            raise Exception(\n",
    "                f\"Couldn't match the header {aHeader} to any header in the database\"\n",
    "            )\n",
    "\n",
    "    print(\"Data types verified\")\n",
    "\n",
    "\n",
    "def date_to_list(x: str or int) -> list or np.NaN:\n",
    "    \"\"\"\n",
    "    This function takes in a date in a string format (if it has unknown value in it like 'xx') or in an 5-integer format and converts it to a list of integers. The date will be in format of 'YYYY-MM-DD'. The function will return a list of integers in the format of [YYYY, MM, DD]. It will return None, if the input is None.\n",
    "    :param x: The date to convert to a list of integers.\n",
    "    :return: A list of integers in the format of [YYYY, MM, DD].\n",
    "    \"\"\"\n",
    "    if x is None or pd.isna(x):\n",
    "        return np.NaN\n",
    "    elif type(x) is int:\n",
    "        return list(\n",
    "            map(\n",
    "                int,\n",
    "                pd.to_datetime(arg=x, errors=\"ignore\", unit=\"D\", origin=\"1899-12-30\")\n",
    "                .strftime(\"%Y-%m-%d\")\n",
    "                .split(\"-\"),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        new_list: list = []\n",
    "        for i, element in enumerate(str(x).replace(\"/\", \"-\").split(\"-\")[::-1]):\n",
    "            try:\n",
    "                new_list.append(int(element))\n",
    "            except ValueError:\n",
    "                new_list.append(element)\n",
    "        return new_list\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# 1. Reading the Excel file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "The dates in Excel file have a format of 5-digit numbers, which are the number of days since 00/00/1900 (or 1899-12-30). We need to convert them to a date format.\n",
    "Note that dates before 1900 will be an exact string representation of the exact date.\n",
    "For some reason, I couldn't use `converters` to convert the dates or `dtyps` while reading the Excel file, so I had to use `applymap` to convert them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = glob(path.normpath(fs.InputPath.new_data + \"/*.xlsx\"))[0]\n",
    "print(f\"Read the file: {file}\")\n",
    "na_values = [\n",
    "    \"NA\",\n",
    "    \"N/A\",\n",
    "    \"na\",\n",
    "    \"n/a\",\n",
    "    \"NULL\",\n",
    "    \"null\",\n",
    "    \"not documented\",\n",
    "    \"Not documented\",\n",
    "    \"Not Documented\",\n",
    "    \"nan\",\n",
    "    \"NAN\",\n",
    "    \"None\",\n",
    "    \"none\",\n",
    "]\n",
    "df = pd.read_excel(\n",
    "    file,\n",
    "    index_col=None,\n",
    "    header=0,\n",
    "    na_values=set(na_values),\n",
    "    # true_values=('Yes', 'yes', 'TRUE', 'True', 'true'),\n",
    "    # false_values=('No', 'no', 'FALSE', 'False', 'false'),\n",
    ")\n",
    "\n",
    "# This is used just in case if there are any spaces in the beginning or the end of the string and not being cought by the na_values\n",
    "df = df.map(lambda x: x.strip() if type(x) is str else x)\n",
    "df.replace(na_values, NaN, inplace=True)\n",
    "\n",
    "# Topic: converting the dates to a date format,\n",
    "# Converting the dates to a date format and splitting them into a year, month, and day list.\n",
    "# This is done to make it easier to convert them to json later on.\n",
    "date_cols = list(df.columns[df.columns.str.contains(r\"date\", re.IGNORECASE, re.I)])\n",
    "df.loc[:, date_cols] = df.loc[:, date_cols].map(date_to_list)\n",
    "\n",
    "display(date_cols)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# 2. Changing some data values to a standard\n",
    "\n",
    "Below we will be using `df.loc[index, ('header')]` rather than `df[header][index]` since it is safer and faster.\n",
    "Https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
    "\n",
    "Note that we are using `replace` method rather than using for loops and if statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# below, I implemented {while loop -> try / except a -> match case} to make sure all statements run independently of one another and the try/except would still catch any errors\n",
    "i = 0\n",
    "modified_cols = []\n",
    "while True:\n",
    "    try:\n",
    "        match i:\n",
    "            # PersonInfo\n",
    "            case 0:\n",
    "                df[\"Gender\"].replace([\"Male\", \"Female\"], [1, 0], inplace=True)\n",
    "                modified_cols.append(\"Gender\")\n",
    "                i += 1\n",
    "            case 1:\n",
    "                df[\"Ethnicity\"].replace(\n",
    "                    [\"Non-Spanish; Non-Hispanic\", \"Spanish; Hispanic\"],\n",
    "                    [0, 1],\n",
    "                    inplace=True,\n",
    "                )\n",
    "                modified_cols.append(\"Ethnicity\")\n",
    "                i += 1\n",
    "\n",
    "            # SampleInfo\n",
    "            # CancerInfo\n",
    "            case 2:\n",
    "                df[\"Diagnosis\"].replace(\n",
    "                    [\n",
    "                        \"Non Small Cell Lung Cancer\",\n",
    "                        \"Small Cell Lung Cancer\",\n",
    "                        \"Breast Cancer\",\n",
    "                    ],\n",
    "                    [\"Non Small Cell Lung\", \"Small Cell Lung\", \"Breast\"],\n",
    "                    inplace=True,\n",
    "                )\n",
    "                modified_cols.append(\"Diagnosis\")\n",
    "                i += 1\n",
    "            case 3:\n",
    "                df[\"IHC Assay ER\"].replace(\n",
    "                    [\"positive\", \"negative\"], [1, 0], inplace=True\n",
    "                )\n",
    "                modified_cols.append(\"IHC Assay ER\")\n",
    "                i += 1\n",
    "            case 4:\n",
    "                df[\"IHC Assay PR\"].replace(\n",
    "                    [\"positive\", \"negative\"], [1, 0], inplace=True\n",
    "                )\n",
    "                modified_cols.append(\"IHC Assay PR\")\n",
    "                i += 1\n",
    "            case 5:\n",
    "                df[\"IHC Assay HER-2\"].replace(\n",
    "                    [\"Not Amplified\", \"Equivocal\", \"Amplified\"],\n",
    "                    [-1, 0, 1],\n",
    "                    inplace=True,\n",
    "                )\n",
    "                modified_cols.append(\"IHC Assay HER-2\")\n",
    "                i += 1\n",
    "            case 6:\n",
    "                df[\"FISH Test HER-2\"].replace(\n",
    "                    [\"Not Amplified\", \"Equivocal\", \"Amplified\"],\n",
    "                    [-1, 0, 1],\n",
    "                    inplace=True,\n",
    "                )\n",
    "                modified_cols.append(\"FISH Test HER-2\")\n",
    "                i += 1\n",
    "            case 14:\n",
    "                df[\"Stage\"].replace([0], [\"0\"], inplace=True)\n",
    "                modified_cols.append(\"Stage\")\n",
    "                i += 1\n",
    "\n",
    "            # TreatmentInfo\n",
    "            case 7:\n",
    "                df[\"Tissue Exposure\"].replace([\"Yes\", \"No\"], [1, 0], inplace=True)\n",
    "                modified_cols.append(\"Tissue Exposure\")\n",
    "                i += 1\n",
    "            case 8:\n",
    "                df[\"Rad Tissue Exposure\"].replace([\"Yes\", \"No\"], [1, 0], inplace=True)\n",
    "                modified_cols.append(\"Rad Tissue Exposure\")\n",
    "                i += 1\n",
    "            case 9:\n",
    "                df[\"Chemo\"].replace(\n",
    "                    [\"No\", \"Yes\", \"Prior Chemo\", \"Current Chemo\"],\n",
    "                    [0, 1, 2, 3],\n",
    "                    inplace=True,\n",
    "                )\n",
    "                modified_cols.append(\"Chemo\")\n",
    "                i += 1\n",
    "            case 10:\n",
    "                df[\"Hormonal Therapy\"].replace([\"Yes\", \"No\"], [1, 0], inplace=True)\n",
    "                modified_cols.append(\"Hormonal Therapy\")\n",
    "                i += 1\n",
    "            case 11:\n",
    "                df[\"Immunotherapy\"].replace([\"Yes\", \"No\"], [1, 0], inplace=True)\n",
    "                modified_cols.append(\"Immunotherapy\")\n",
    "                i += 1\n",
    "            case 12:\n",
    "                response_columns = list(\n",
    "                    df.columns[\n",
    "                        df.columns.str.contains(\n",
    "                            r\"response to chemo course\", re.IGNORECASE, re.I\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "                # df.loc[:, response_columns] = df.loc[:, response_columns].map(\n",
    "                #     lambda x: x.strip() if type(x) is str else x)  # No need for this\n",
    "                df.loc[:, response_columns] = df.loc[:, response_columns].replace(\n",
    "                    [\n",
    "                        \"CR - Complete Response\",\n",
    "                        \"PR - Partial Response\",\n",
    "                        \"SD - Stable Disease\",\n",
    "                        \"PD - Progressive Disease\",\n",
    "                    ],\n",
    "                    [\"CR\", \"PR\", \"SD\", \"PD\"],\n",
    "                )\n",
    "                modified_cols += response_columns\n",
    "                i += 1\n",
    "            case 13:\n",
    "                course_columns = list(\n",
    "                    df.columns[\n",
    "                        df.columns.str.fullmatch(r\"course \\d{,2}\", re.IGNORECASE, re.I)\n",
    "                    ]\n",
    "                )\n",
    "                df.loc[:, course_columns] = df.loc[:, course_columns].map(\n",
    "                    lambda x: imp.split_strip(x, [\"/\", \";\"]) if type(x) is str else x\n",
    "                )\n",
    "                modified_cols += course_columns\n",
    "                i += 1\n",
    "        print(f\"Case {i - 1} -> Done\")\n",
    "        if i > 14:\n",
    "            break\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"Case {i} -> Error: {e} not found\")\n",
    "        i += 1\n",
    "        pass\n",
    "\n",
    "for col in modified_cols:\n",
    "    print(\"------------------------------------------\")\n",
    "    print(df[col].value_counts(dropna=False))\n",
    "\n",
    "# df.loc[25:30, modified_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# 3. Changing header names to match DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: rewrite the rename function to use RE instead of hard coded names to better find the column names in df.\n",
    "\n",
    "# making sure that specimen type is known\n",
    "# checking the columns for 'specimen type'\n",
    "spec_type_num = re.findall(\n",
    "    r\"\\'specimen\\s*type\\s*\", str(list(df.columns)), flags=re.IGNORECASE\n",
    ")\n",
    "message = f\"A number of {len(spec_type_num)} specimen type column(s) were found,\\ncontinue? (y/n)\"\n",
    "question = False  # initializing\n",
    "\n",
    "# if there is more than one 'Specimen Type'\n",
    "if len(spec_type_num) > 1 and (question := input(message).lower() == \"n\"):\n",
    "    raise Exception(\n",
    "        '1. Figure out which \"specimen type\" each one is\\n2. Re-run the code'\n",
    "    )\n",
    "\n",
    "# Otherwise, if we found a specimen type\n",
    "elif len(spec_type_num) == 1 or question:\n",
    "    target = str(set(df.loc[:, \"Specimen Type\"]))\n",
    "    is_glycan_spec = re.findall(\n",
    "        r\"Malignant \\w*\", target, flags=re.IGNORECASE\n",
    "    ) + re.findall(r\"Normal \\w*\", target, flags=re.IGNORECASE)\n",
    "    if is_glycan_spec:\n",
    "        # Glycan table: Normal Serum, Malignant Serum, Normal Tissue, Malignant Tissue\n",
    "        df.rename(columns={\"Specimen Type\": \"GlycanSpecimenType\"}, inplace=True)\n",
    "    else:\n",
    "        # Galectin table: Serum, Plasma\n",
    "        df.rename(columns={\"Specimen Type\": \"GalectinSpecimenType\"}, inplace=True)\n",
    "\n",
    "df.rename(\n",
    "    columns={\n",
    "        # PersonalInfo Table\n",
    "        \"FWID\": \"ID\",\n",
    "        \"Patient Birth Year\": \"BirthYear\",\n",
    "        \"Smoking History\": \"SmokeHistory\",\n",
    "        \"Gender\": \"Sex\",\n",
    "        # SampleInfo Table\n",
    "        \"Unique Aliquot ID\": \"AliquotID\",\n",
    "        \"Specimen Considered\": \"SpecimenConsideration\",\n",
    "        \"Specimen age in years\": \"SpecimenAge\",\n",
    "        \"Specimen Collection Year\": \"SpecimenCollectionYear\",\n",
    "        \"Patient Age at Collection\": \"PatientAgeAtCollection\",\n",
    "        \"Additional Histology\": \"OtherHistology\",\n",
    "        \"Tissue Site (Histo)\": \"Site\",\n",
    "        \"Specimen Grade\": \"Grade\",\n",
    "        # CancerInfo Table\n",
    "        \"IHC Assay ER\": \"IHC_Assay_ER\",\n",
    "        \"IHC ER %\": \"IHC_ER\",\n",
    "        \"IHC Assay PR\": \"IHC_Assay_PR\",\n",
    "        \"IHC PR %\": \"IHC_PR\",\n",
    "        \"IHC Assay HER-2\": \"IHC_Assay_HER2\",\n",
    "        \"FISH Test HER-2\": \"FISH_Test_HER2\",\n",
    "        # TreatmentInfo Table\n",
    "        \"Tissue Exposure\": \"TissueExposure\",\n",
    "        \"Rad Tissue Exposure\": \"RadTissueExposure\",\n",
    "        \"Hormonal Therapy\": \"HormonalTherapy\",\n",
    "        \"Treatment Status\": \"TreatmentStatus\",\n",
    "        \"Surgery Date\": \"SurgeryDate\",\n",
    "        \"Patient Chemo\": \"Chemo\",\n",
    "        # Galectins\n",
    "        \"Gal-1\": \"Gal1\",\n",
    "        \"Gal-3\": \"Gal3\",\n",
    "        \"Gal-7\": \"Gal7\",\n",
    "        \"Gal-8\": \"Gal8\",\n",
    "        \"Gal-9\": \"Gal9\",\n",
    "        # Glycan\n",
    "        # Genes\n",
    "        # this is under the assumption that a single file will not have duplicate names\n",
    "        \"ABLI\": \"ABL1\",\n",
    "        \"ABL\": \"ABL1\",\n",
    "        \"AKT\": \"AKT1\",\n",
    "        \"AKTI\": \"AKT1\",\n",
    "        \"CDHI\": \"CDH1\",\n",
    "        \"CDH\": \"CDH1\",\n",
    "        \"CSFIR\": \"CSF1R\",\n",
    "        \"CTNNBI\": \"CTNNB1\",\n",
    "        \"CTNNB\": \"CTNNB1\",\n",
    "        \"FGFRI\": \"FGFR1\",\n",
    "        \"GNAII\": \"GNA11\",\n",
    "        \"GNA\": \"GNA11\",\n",
    "        \"HNFIA\": \"HNF1A\",\n",
    "        \"MLHI\": \"MLH1\",\n",
    "        \"NOTCHI\": \"NOTCH1\",\n",
    "        \"NPMI\": \"NPM1\",\n",
    "        \"NPM\": \"NPM1\",\n",
    "        \"PTPN\": \"PTPN11\",\n",
    "        \"PTPNII\": \"PTPN11\",\n",
    "        \"SMARCBI\": \"SMARCB1\",\n",
    "        \"SMARCB\": \"SMARCB1\",\n",
    "        \"SDK\": \"SDK11\",\n",
    "        \"SDKII\": \"SDK11\",\n",
    "        \"RBI\": \"RB1\",\n",
    "        # Pathways\n",
    "        \"jak/stat\": \"JAK_STAT\",\n",
    "        \"p53\": \"P53\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is to fix a specific error in the data\n",
    "run_this = False\n",
    "run_this = (\n",
    "    input(\"Do you want to run this cell\\nto modify some data? (y/n)\").lower() == \"y\"\n",
    ")\n",
    "if run_this:\n",
    "    # df.loc[199, 'IHC_PR']=np.NaN\n",
    "    # df.loc[:, 'Grade'].replace(['U'], [0], inplace=True)\n",
    "    # df.loc[:, 'Grade'] = df.loc[:, 'Grade'].map(int, na_action='ignore')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if we ended up with duplicated column names from last step\n",
    "columns = df.columns\n",
    "temp = False\n",
    "for i in range(len(columns)):\n",
    "    for j in range(i + 1, len(columns)):\n",
    "        if columns[i] == columns[j]:\n",
    "            temp = True\n",
    "            print(f\"Columns '{columns[i]}' and '{columns[j]}' have the same name\")\n",
    "if temp:\n",
    "    raise Exception(\n",
    "        \"You have duplicate columns that you need to either fix or merge them\"\n",
    "    )\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# 4. Dividing the Data Frame\n",
    "We use `df2 = df[['col1', 'col2']].copy()` to choose related columns and separate them. Although we can ignore the `.copy()`, using that would avoid a `SettingWithCopyWarning` error. The inner square brackets define a Python list with column names, whereas the outer brackets are used to select the data from a pandas DataFrame as seen.\n",
    "Also, depending on the number of columns in your original dataframe, it might be more succinct to express this using a drop (this will also create a copy by default), like `df2 = df.drop('col3', axis=1)`\n",
    "\n",
    "To merge separate dataframes together, we use `pd.concat(dfList)`. As the default argument `axis = 0`, this will **union** the dataframes in a vertical order and the result will be like: (df1.col1 + df2.col1 + ...), (df1.col2 + df2.col2 + ...), etc.\n",
    "In other words, `axis = 0` means that all the values from first df first col will be added to the result df first col, then add all the values from second df first col will be added to the result df first col, and so on.\n",
    "If we want to merge the dataframes in a horizontal order, we must specify `axis = 1`. This will result in a dataframe looking like: df1.col1, df1.col2, ..., df2.col1, df2.col2, ..., etc.\n",
    "There is also an argument called `ignore_index` that by default is set to false. If set True, the result dataframe will not have any headers. Read source for more options explanation.\n",
    "\n",
    "source:\n",
    "https://pandas.pydata.org/docs/getting_started/intro_tutorials/03_subset_data.html\n",
    "https://sparkbyexamples.com/pandas/pandas-create-new-dataframe-by-selecting-specific-columns/\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping any row that doesn't have an ID\n",
    "df.dropna(subset=[\"ID\"], inplace=True)\n",
    "\n",
    "# dataframes related to the database\n",
    "PersonInfo = slice_df(df, \"PersonInfo\")\n",
    "CancerInfo = slice_df(df, \"CancerInfo\")\n",
    "SampleInfo = slice_df(df, \"SampleInfo\")\n",
    "Galectin = slice_df(df, \"Galectin\")\n",
    "Glycan = slice_df(df, \"Glycan\")\n",
    "Genes = slice_df(df, \"Genes\")\n",
    "Pathways = slice_df(df, \"Pathways\")\n",
    "try:\n",
    "    TreatmentInfo = slice_df(df, \"TreatmentInfo\").drop(\n",
    "        columns=[\"SurgeryDate\"]\n",
    "    )  # SurgeryDate will be added to JSON \"TreatmentCourse\" later\n",
    "except KeyError:\n",
    "    print(\"No 'SurgeryDate' column was found\")\n",
    "    TreatmentInfo = slice_df(df, \"TreatmentInfo\")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# 5. Adding JSON values\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html?utm_source=pocket_saves\n",
    "https://github.com/ijl/orjson\n",
    "\n",
    "## 5.1 Gene Mutations\n",
    "To improve the speed if needed, you can look into using package \"swiftly\", Numba, or CUPY. See obsidian notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: Verify the mutations using hgfv library. You may want to add it to the genes_str_json function\n",
    "# todo: see if np.vectorize() have any impact on function speed\n",
    "# Victorizing the function, assuming it will make it run faster\n",
    "# vec_func = np.vectorize(str_to_dict)\n",
    "\n",
    "# creating a pattern to match the mutations but not ID and EdgeCase\n",
    "cols_to_apply_g = ~Genes.columns.isin([\"ID\", \"EdgeCase\"])\n",
    "\n",
    "# Applying the function to the dataframe to convert the string mutations to json format\n",
    "Genes.loc[:, cols_to_apply_g] = Genes.loc[:, cols_to_apply_g].applymap(\n",
    "    str_to_dict, data_json_name=\"mutations\", na_action=\"ignore\"\n",
    ")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 5.2 Pathways\n",
    "We will keep this simple just having the count of mutations. This is exactly what is reported to us. To do this, I will use the same helper function `genes_str_json` that I used for the Genes table.\n",
    "In the future, we may want to add the genes that are related to the pathway in the json value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_apply_p = ~Pathways.columns.isin([\"ID\", \"EdgeCase\"])\n",
    "Pathways.loc[:, cols_to_apply_p] = Pathways.loc[:, cols_to_apply_p].applymap(\n",
    "    str_to_dict,\n",
    "    keep_original=True,\n",
    "    na_action=\"ignore\",\n",
    "    include_count=False,\n",
    "    cast_data=int,\n",
    ")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 5.3 TreatmentInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixme: check to see if you need to split TreatmentStatus too?\n",
    "# todo: find a way to sort courses better in the json str like {course01 : {treatment: [], chemo: xxx, startdate: [], response: xxx},...}\n",
    "\n",
    "# to get all the columns that have the word \"course\" or \"SurgeryDate\" or \"TreatmentStatus\"\n",
    "pattern = r\"course\\s+\\d+|SurgeryDate|TreatmentStatus\"\n",
    "\n",
    "# Getting the columns that match the pattern from Excel file dataframe\n",
    "treatment_json_col = list(\n",
    "    df.columns[df.columns.str.contains(pattern, re.IGNORECASE, re.I)]\n",
    ")\n",
    "\n",
    "# Getting all the data in a single dataframe\n",
    "treatment_json_df = sub_df(df, treatment_json_col)\n",
    "\n",
    "# this gives back a list of strings that we are assigning to \"TreatmentStatus\"\n",
    "# Creating json values for each row like {col1: val1, col2: val2, ...}\n",
    "# if you don't want to have a dictionary, just remove [json.loads(x) for x in ...] and keep the rest (the ... code)\n",
    "if type(treatment_json_df) == pd.DataFrame:\n",
    "    if del_null_values := True:\n",
    "        TreatmentInfo[\"TreatmentCourses\"] = [\n",
    "            clean_nulls(x)\n",
    "            for x in (\n",
    "                treatment_json_df.to_json(orient=\"records\", lines=True).splitlines()\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        TreatmentInfo[\"TreatmentCourses\"] = [\n",
    "            json.loads(x)\n",
    "            for x in treatment_json_df.to_json(\n",
    "                orient=\"records\", lines=True\n",
    "            ).splitlines()\n",
    "        ]\n",
    "else:\n",
    "    print(\"No JSON values for 'TreatmentInfo' table was found\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 5.4 Verifying the JSON values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Note that if del_null_values is True, then you might see treatmentCourses empty, because it was removed.\n",
    "if treatment_json_df is not None:\n",
    "    print(f\"TreatmentInfo\\n{TreatmentInfo.loc[0, 'TreatmentCourses']}\")\n",
    "    print(\"----------------------------------------------------\")\n",
    "\n",
    "print(f\"Genes\\n{Genes.loc[0, cols_to_apply_g]}\")\n",
    "print(\"----------------------------------------------------\")\n",
    "print(f\"Pathways\\n{Pathways.loc[0, cols_to_apply_p]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "# 6. Dropping Duplicates\n",
    "Based on table's primary key. The function will reset the index in the background.\n",
    "\n",
    "source: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: use a loop to do this rather than hard coding the table names\n",
    "PersonInfo = dropDup(\"PersonInfo\")\n",
    "Pathways = dropDup(\"Pathways\", ignore_columns=[\"EdgeCase\"])\n",
    "Genes = dropDup(\"Genes\", ignore_columns=[\"EdgeCase\"])\n",
    "\n",
    "# fixme: remove additional arguments when you get access to AliquotIDs\n",
    "TreatmentInfo = dropDup(\"TreatmentInfo\", ignore_columns=[\"AliquotID\"])\n",
    "CancerInfo = dropDup(\"CancerInfo\", ignore_columns=[\"AliquotID\"])\n",
    "SampleInfo = dropDup(\"SampleInfo\", ignore_columns=[\"AliquotID\"])\n",
    "Galectin = dropDup(\"Galectin\", ignore_columns=[\"AliquotID\"])\n",
    "Glycan = dropDup(\"Glycan\", ignore_columns=[\"AliquotID\", \"GlycanSpecimenType\"])\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## 6.1 Saving the result to a dictionary and removing empty tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: use a loop to do this rather than hard coding the table names\n",
    "df_dict: dict = {\n",
    "    \"PersonInfo\": PersonInfo,\n",
    "    \"CancerInfo\": CancerInfo,\n",
    "    \"SampleInfo\": SampleInfo,\n",
    "    \"TreatmentInfo\": TreatmentInfo,\n",
    "    \"Galectin\": Galectin,\n",
    "    \"Glycan\": Glycan,\n",
    "    \"Genes\": Genes,\n",
    "    \"Pathways\": Pathways,\n",
    "}\n",
    "\n",
    "# Removing Empty tables\n",
    "# Getting all the table names from the database\n",
    "tableNames = set(pk_schema[\"table_name\"])\n",
    "# Setting the ignore_columns to ignore when comparing the primary keys\n",
    "ignore_columns = [\"EdgeCase\"]\n",
    "\n",
    "for aTable in tableNames:\n",
    "    # 1.1 list the primary keys of aTable\n",
    "    aTable_pk = set(pk_schema[pk_schema[\"table_name\"] == aTable][\"col_name\"])\n",
    "\n",
    "    # making sure that ignore_columns is a list if not None\n",
    "    if ignore_columns is not None:\n",
    "        aTable_pk = aTable_pk - set(ignore_columns)\n",
    "\n",
    "    # 2. get the primary keys of that table from df_dict and deleting that table if it is empty\n",
    "    if set(df_dict[aTable].columns) - set(aTable_pk) == set():\n",
    "        del df_dict[aTable]\n",
    "        print(f\"Table {aTable} -> Empty and has been removed\")\n",
    "\n",
    "# todo: remove empty rows from each table that only have primary keys in them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "# 7. Verifying & Fixing data Types\n",
    "\n",
    "## what is the difference between `check_dtype` function and `verify_dtype` ?\n",
    "check_dtype function checks the data types of a specific column in a pandas DataFrame and raises an exception if any data in that column does not match the specified data type or is not of the specified ignored data type.\n",
    "\n",
    "verify_dtype function verifies the data types of all the columns in a pandas DataFrame against a specified DataFrame of column data types. It compares the data types of each column in the given DataFrame (aTable) with the corresponding column in the specified DataFrame of column data types (aDB_schema). If the data type of a column in aTable does not match the corresponding column in aDB_schema, it raises an exception.\n",
    "\n",
    "In summary check_dtype checks for the datatype of one specific column, while verify_dtype checks for the datatype of all columns in a DataFrame against another DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for key in df_dict.keys():\n",
    "        if len(df_dict[key].columns) > 1:\n",
    "            # print(f\"Table {key} -> \", end='')\n",
    "            verify_dtype(df_dict[key], db_schema, key)\n",
    "            # print(f\"Data types verified\")\n",
    "\n",
    "    print(\"\\nAll good!\\n\")\n",
    "except Exception as e:\n",
    "    raise Exception(\n",
    "        f\"Error: {e} \\n\\n======= Please fix the problem in the IMPORT FILE cell and run ALL cells again =======\"\n",
    "    )\n",
    "print(\n",
    "    \"===== Note that JSON values datatype were verified as dictionaries =====\\n\\t\\t\\t===== In-dict dataTypes were not verified =====\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "# 8. Pickling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = True\n",
    "for name in [nameof(df), nameof(df_dict)]:\n",
    "    test = glob(join(PklPath.data_importer, f\"{name}.pkl\"))\n",
    "    if test:\n",
    "        overwrite = input(f\"{name} exists, Overwrite? (y/n)\").lower() == \"y\"\n",
    "        if not overwrite:\n",
    "            break\n",
    "        # todo: make it check each one independently\n",
    "\n",
    "if overwrite:\n",
    "    with open(join(PklPath.data_importer, \"df.pkl\"), \"wb\") as pickle_out:\n",
    "        pickle.dump(df, pickle_out)\n",
    "\n",
    "    with open(join(PklPath.data_importer, \"df_dict.pkl\"), \"wb\") as pickle_out:\n",
    "        pickle.dump(df_dict, pickle_out)\n",
    "else:\n",
    "    raise Exception(\"Allow overwriting, or rename\")\n",
    "\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "notebooks//ipynb,_notebooks//py:light"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
